import os
from dotenv import load_dotenv
from huggingface_hub import login

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô .env
load_dotenv()

# ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Token
hf_token = os.getenv("HF_TOKEN_KEY")
login(hf_token)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

import torch

# ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
user_input = "‡∏â‡∏±‡∏ô‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ"

# ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó
prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏π‡πÅ‡∏•‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô 
‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à:

"{user_input}"
"""

# Tokenize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

# ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• generate ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
outputs = model.generate(
    **inputs,
    max_new_tokens=200,        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Token ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
    temperature=0.7,           # ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (0-1)
    top_p=0.9,                 # ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
)

# ‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≠‡∏ö‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("\nü§ñ Bot ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏ß‡πà‡∏≤:\n")
print(response)
